{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from cluster_g_logit_init_acceptpsi import *\n",
    "\n",
    "\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import expit\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering  # Add this import\n",
    "\n",
    "def load_model_essentials(base_path='/Users/sarahurbut/Dropbox (Personal)/data_for_running/'):\n",
    "    \"\"\"\n",
    "    Load all essential components\n",
    "    \"\"\"\n",
    "    print(\"Loading components...\")\n",
    "    \n",
    "    # Load large matrices\n",
    "    Y = torch.load(base_path + 'Y_tensor.pt')\n",
    "    E = torch.load(base_path + 'E_matrix.pt')\n",
    "    G = torch.load(base_path + 'G_matrix.pt')\n",
    "    \n",
    "    # Load other components\n",
    "    essentials = torch.load(base_path + 'model_essentials.pt')\n",
    "    \n",
    "    print(\"Loaded all components successfully!\")\n",
    "    \n",
    "    return Y, E, G, essentials\n",
    "\n",
    "# Load and initialize model:\n",
    "Y, E, G, essentials = load_model_essentials()\n",
    "\n",
    "\n",
    "\n",
    "def subset_data(Y, E, G, n_samples=50000, seed=42):\n",
    "    \"\"\"\n",
    "    Subset the data to n_samples individuals while maintaining consistency\n",
    "    \n",
    "    Args:\n",
    "        Y: tensor of shape [N, D, T]\n",
    "        E: tensor of shape [N, D]\n",
    "        G: tensor of shape [N, P]\n",
    "        n_samples: number of individuals to keep\n",
    "        seed: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Y_sub, E_sub, G_sub: subsetted tensors\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Get total number of individuals\n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    # Randomly select n_samples indices\n",
    "    indices = torch.randperm(N)[:n_samples]\n",
    "    \n",
    "    # Subset all matrices using the same indices\n",
    "    Y_sub = Y[indices]\n",
    "    E_sub = E[indices]\n",
    "    G_sub = G[indices]\n",
    "    \n",
    "    print(f\"Original shapes: Y={Y.shape}, E={E.shape}, G={G.shape}\")\n",
    "    print(f\"New shapes: Y={Y_sub.shape}, E={E_sub.shape}, G={G_sub.shape}\")\n",
    "    \n",
    "    return Y_sub, E_sub, G_sub, indices\n",
    "\n",
    "# Subset the data\n",
    "Y_100k, E_100k, G_100k, indices = subset_data(Y, E, G, n_samples=10000)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Initialize model with subsetted data\n",
    "\n",
    "# When initializing the model:\n",
    "original_G = G_100k.clone().detach()  # Store the original G - proper tensor copy\n",
    "\n",
    "# Now in your batch run, load and verify:\n",
    "initial_psi = torch.load('initial_psi_400k.pt')\n",
    "initial_clusters = torch.load('initial_clusters_400k.pt')\n",
    "\n",
    "original_cluster_sizes = {}\n",
    "unique, counts = np.unique(initial_clusters, return_counts=True)\n",
    "for k, count in zip(unique, counts):\n",
    "    original_cluster_sizes[k] = count\n",
    "print(\"\\nOriginal cluster sizes:\")\n",
    "for k, count in original_cluster_sizes.items():\n",
    "    print(f\"Cluster {k}: {count} diseases\")\n",
    "print(\"Initial psi stats:\")\n",
    "print(f\"Shape: {initial_psi.shape}\")\n",
    "print(f\"Range: [{initial_psi.min():.2f}, {initial_psi.max():.2f}]\")\n",
    "print(f\"Number of positive values: {(initial_psi > 0).sum().item()}\")\n",
    "\n",
    "\n",
    "### did not run with the new initial, we used built in clusters from the model init with the 1/2 clustering appraohc. \n",
    "torch.manual_seed(222)\n",
    "np.random.seed(222)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(222)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Now in your batch run, load and verify:\n",
    "initial_psi = torch.load('initial_psi_400k.pt')\n",
    "initial_clusters = torch.load('initial_clusters_400k.pt')\n",
    "\n",
    "model = AladynSurvivalFixedKernelsAvgLoss_clust_logitInit_psitest(\n",
    "    N=Y_100k.shape[0],\n",
    "    D=Y_100k.shape[1],\n",
    "    T=Y_100k.shape[2],\n",
    "    K=essentials['K'],\n",
    "    P=essentials['P'],\n",
    "    G=G_100k,\n",
    "    Y=Y_100k,\n",
    "    prevalence_t=essentials['prevalence_t']\n",
    ")\n",
    "\n",
    "# Initialize with saved parameters\n",
    "model.initialize_params(true_psi=initial_psi)\n",
    "model.clusters = initial_clusters\n",
    "print(\"\\nVerifying cluster sizes match:\")\n",
    "new_unique, new_counts = np.unique(model.clusters, return_counts=True)\n",
    "for k, count in zip(new_unique, new_counts):\n",
    "    print(f\"Cluster {k}: {count} diseases (Original: {original_cluster_sizes[k]} diseases)\")\n",
    "\n",
    "# Verify psi values match\n",
    "print(\"\\nVerifying psi values match:\")\n",
    "print(f\"Psi values match: {torch.allclose(model.psi, initial_psi)}\")\n",
    "def calculate_calibration_stats(model, Y):\n",
    "    \"\"\"Calculate calibration stats for a model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        predicted = model.forward()\n",
    "        pi_pred = predicted[0] if isinstance(predicted, tuple) else predicted\n",
    "        pi_pred = pi_pred.cpu().detach().numpy()\n",
    "        Y_np = Y.cpu().detach().numpy() if torch.is_tensor(Y) else Y\n",
    "        \n",
    "        # Convert to numpy and calculate means\n",
    "        observed_risk = Y_np.mean(axis=0).flatten()\n",
    "        predicted_risk = pi_pred.mean(axis=0).flatten()\n",
    "        \n",
    "        scale_factor = np.mean(observed_risk) / np.mean(predicted_risk)\n",
    "        calibrated_risk = predicted_risk * scale_factor\n",
    "        \n",
    "        ss_res = np.sum((observed_risk - calibrated_risk) ** 2)\n",
    "        ss_tot = np.sum((observed_risk - np.mean(observed_risk)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return r2, scale_factor, observed_risk, predicted_risk, calibrated_risk\n",
    "\n",
    "\n",
    "\n",
    "# 1. Global R² (we know it's 0.70)\n",
    "print(f\"Global model R² (known): 0.70\")\n",
    "\n",
    "\n",
    "r2_global, scale_global, obs_global, pred_global, cal_global = calculate_calibration_stats(model, Y_100k)\n",
    "print(f\"\\nBatch model with global psi:\")\n",
    "print(f\"R²: {r2_global:.3f}\")\n",
    "print(f\"Calibration scale factor: {scale_global:.3f}\")\n",
    "\n",
    "\n",
    "# Plot both versions:\n",
    "model.plot_genetic_scores(original_G)\n",
    "print(model.G[:,3].mean())\n",
    "\n",
    "print(original_G[:,3].mean())\n",
    "\n",
    "\n",
    "# Check G matrix scaling\n",
    "G_mean = model.G.mean(dim=0)\n",
    "G_std = model.G.std(dim=0)\n",
    "print(\"\\nG matrix scaling check:\")\n",
    "print(f\"Mean should be ~0: {G_mean.abs().max().item():.6f}\")\n",
    "print(f\"Std should be ~1: {(G_std - 1).abs().max().item():.6f}\")\n",
    "\n",
    "\n",
    "initial_gamma = model.gamma.detach().clone()\n",
    "initial_phi = model.phi.detach().clone()\n",
    "initial_lambda = model.lambda_.detach().clone()\n",
    "initial_psi = model.psi.detach().clone()\n",
    "\n",
    "\n",
    "history = model.fit(E_100k, num_epochs=2, learning_rate=1e-4, lambda_reg=1e-2)\n",
    "\n",
    "### plotting\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history['loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot gradients\n",
    "ax2.plot(history['max_grad_lambda'], label='Lambda')\n",
    "ax2.plot(history['max_grad_phi'], label='Phi')\n",
    "ax2.plot(history['max_grad_gamma'], label='Gamma')\n",
    "ax2.plot(history['max_grad_psi'], label='Psi')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Max Gradient Magnitude')\n",
    "ax2.set_title('Parameter Gradients')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "disease_names=essentials['disease_names']\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert gamma tensors to numpy arrays\n",
    "initial_gamma_np = initial_gamma.detach().numpy()\n",
    "final_gamma_np = model.gamma.detach().numpy()\n",
    "\n",
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot initial gamma\n",
    "sns.heatmap(initial_gamma_np, ax=ax1, cmap='RdBu_r', center=0)\n",
    "ax1.set_title('Initial Gamma')\n",
    "\n",
    "# Plot final gamma\n",
    "sns.heatmap(final_gamma_np, ax=ax2, cmap='RdBu_r', center=0)\n",
    "ax2.set_title('Final Gamma')\n",
    "\n",
    "# Add a title to the figure\n",
    "plt.suptitle('Comparison of Initial vs Final Gamma Values', fontsize=16)\n",
    "\n",
    "# You can also add a colorbar\n",
    "plt.tight_layout()\n",
    "\n",
    "# To see the actual difference, you can also create a difference heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "diff = final_gamma_np - initial_gamma_np\n",
    "sns.heatmap(diff, cmap='RdBu_r', center=0)\n",
    "plt.title('Difference (Final - Initial Gamma)')\n",
    "plt.show()\n",
    "\n",
    "# Print some summary statistics\n",
    "print(f\"Mean absolute difference: {np.abs(diff).mean():.4f}\")\n",
    "print(f\"Max absolute difference: {np.abs(diff).max():.4f}\")\n",
    "print(f\"Standard deviation of differences: {np.std(diff):.4f}\")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert phi tensors to numpy arrays\n",
    "initial_phi_np = initial_phi.detach().numpy()  # Shape: (K, D, T)\n",
    "final_phi_np = model.phi.detach().numpy()\n",
    "\n",
    "# Take mean over time dimension\n",
    "initial_phi_mean = initial_phi_np.mean(axis=2)  # Shape: (K, D)\n",
    "final_phi_mean = final_phi_np.mean(axis=2)\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot initial phi (averaged over time)\n",
    "sns.heatmap(initial_phi_mean, ax=ax1, cmap='RdBu_r', center=0)\n",
    "ax1.set_title('Initial Phi (Mean over Time)')\n",
    "ax1.set_xlabel('Disease')\n",
    "ax1.set_ylabel('Signature')\n",
    "\n",
    "# Plot final phi (averaged over time)\n",
    "sns.heatmap(final_phi_mean, ax=ax2, cmap='RdBu_r', center=0)\n",
    "ax2.set_title('Final Phi (Mean over Time)')\n",
    "ax2.set_xlabel('Disease')\n",
    "ax2.set_ylabel('Signature')\n",
    "\n",
    "plt.suptitle('Comparison of Initial vs Final Phi Values (Averaged over Time)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot difference\n",
    "plt.figure(figsize=(10, 8))\n",
    "diff_mean = final_phi_mean - initial_phi_mean\n",
    "sns.heatmap(diff_mean, cmap='RdBu_r', center=0)\n",
    "plt.title('Difference in Phi (Final - Initial), Mean over Time')\n",
    "plt.xlabel('Disease')\n",
    "plt.ylabel('Signature')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics for both full tensor and time-averaged differences\n",
    "print(\"Full tensor statistics:\")\n",
    "diff_full = final_phi_np - initial_phi_np\n",
    "print(f\"Mean absolute difference: {np.abs(diff_full).mean():.4f}\")\n",
    "print(f\"Max absolute difference: {np.abs(diff_full).max():.4f}\")\n",
    "print(f\"Standard deviation of differences: {np.std(diff_full):.4f}\")\n",
    "\n",
    "print(\"\\nTime-averaged statistics:\")\n",
    "print(f\"Mean absolute difference: {np.abs(diff_mean).mean():.4f}\")\n",
    "print(f\"Max absolute difference: {np.abs(diff_mean).max():.4f}\")\n",
    "print(f\"Standard deviation of differences: {np.std(diff_mean):.4f}\")\n",
    "\n",
    "# Optionally, look at temporal variation\n",
    "temporal_std = np.std(diff_full, axis=2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(temporal_std, cmap='viridis')\n",
    "plt.title('Standard Deviation of Changes Across Time')\n",
    "plt.xlabel('Disease')\n",
    "plt.ylabel('Signature')\n",
    "plt.show()\n",
    "Y_global=Y\n",
    "\n",
    "Y_global.shape\n",
    "# 1. Get predictions and actual values\n",
    "predicted = model.forward()\n",
    "pi_pred = predicted[0] if isinstance(predicted, tuple) else predicted\n",
    "pi_pred = pi_pred.cpu().detach().numpy()\n",
    "Y = model.Y.cpu().detach().numpy()\n",
    "\n",
    "# 2. Calculate marginal risks directly\n",
    "# Assuming dimensions are: [N, D, T] for both Y and pi_pred\n",
    "observed_risk = Y.mean(axis=0).flatten()  # average across individuals\n",
    "predicted_risk = pi_pred.mean(axis=0).flatten()\n",
    "\n",
    "# 3. Apply calibration\n",
    "scale_factor = np.mean(observed_risk) / np.mean(predicted_risk)\n",
    "calibrated_risk = predicted_risk * scale_factor\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Original predictions\n",
    "plt.subplot(121)\n",
    "plt.scatter(observed_risk, predicted_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Original Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Predicted Risk')\n",
    "\n",
    "# Calibrated predictions\n",
    "plt.subplot(122)\n",
    "plt.scatter(observed_risk, calibrated_risk, alpha=0.5)\n",
    "plt.plot([0, 0.02], [0, 0.02], 'r--')  # y=x line\n",
    "plt.title('Calibrated Predictions')\n",
    "plt.xlabel('Observed Risk')\n",
    "plt.ylabel('Calibrated Risk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean observed risk: {np.mean(observed_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (original): {np.mean(predicted_risk):.6f}\")\n",
    "print(f\"Mean predicted risk (calibrated): {np.mean(calibrated_risk):.6f}\")\n",
    "print(f\"Calibration scale factor: {scale_factor:.3f}\")\n",
    "\n",
    "\n",
    "ss_res = np.sum((observed_risk - calibrated_risk) ** 2)\n",
    "ss_tot = np.sum((observed_risk - np.mean(observed_risk)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"R^2: {r2:.3f}\")\n",
    "def plot_signature_top_diseases_centered(model, disease_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Show top diseases for each signature, centered relative to prevalence\n",
    "    \"\"\"\n",
    "    # Get phi and prevalence\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # For each signature, get top diseases\n",
    "    for k in range(phi_avg.shape[0]):\n",
    "        scores = phi_avg[k, :]\n",
    "        top_indices = np.argsort(scores)[-n_top:][::-1]\n",
    "        \n",
    "        print(f\"\\nTop {n_top} diseases in Signature {k} (relative to baseline):\")\n",
    "        for idx in top_indices:\n",
    "            avg_effect = scores[idx]\n",
    "            temporal_std = np.std(phi_centered[k, idx, :])\n",
    "            # Convert to odds ratio for interpretability\n",
    "            odds_ratio = np.exp(avg_effect)\n",
    "            print(f\"{disease_names[idx]}: effect={avg_effect:.3f} (OR={odds_ratio:.2f}), std={temporal_std:.3f}\")\n",
    "\n",
    "# Run visualization\n",
    "plot_signature_top_diseases_centered(model, essentials['disease_names'])\n",
    "def compare_disease_rankings(model, disease_names, n_top=10):\n",
    "    \"\"\"\n",
    "    Compare initial vs final disease rankings for each signature\n",
    "    \"\"\"\n",
    "    # Get initial rankings from psi\n",
    "    psi = model.psi.detach().numpy()  # Shape: (K, D)\n",
    "    \n",
    "    # Get final rankings from centered phi\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # Compare rankings for each signature\n",
    "    for k in range(phi_avg.shape[0]):\n",
    "        print(f\"\\nSignature {k}:\")\n",
    "        \n",
    "        # Get initial top diseases from psi\n",
    "        initial_scores = psi[k, :]\n",
    "        initial_top = np.argsort(initial_scores)[-n_top:][::-1]\n",
    "        \n",
    "        # Get final top diseases from phi\n",
    "        final_scores = phi_avg[k, :]\n",
    "        final_top = np.argsort(final_scores)[-n_top:][::-1]\n",
    "        \n",
    "        print(\"\\nInitial top diseases:\")\n",
    "        for i, idx in enumerate(initial_top):\n",
    "            print(f\"{i+1}. {disease_names[idx]}: {initial_scores[idx]:.3f}\")\n",
    "            \n",
    "        print(\"\\nFinal top diseases:\")\n",
    "        for i, idx in enumerate(final_top):\n",
    "            print(f\"{i+1}. {disease_names[idx]}: {final_scores[idx]:.3f}\")\n",
    "            \n",
    "        # Calculate rank changes\n",
    "        initial_ranks = {disease: rank for rank, disease in enumerate(initial_top)}\n",
    "        final_ranks = {disease: rank for rank, disease in enumerate(final_top)}\n",
    "        \n",
    "        # Find diseases that changed ranks significantly\n",
    "        changed_diseases = set(initial_top) | set(final_top)\n",
    "        for disease in changed_diseases:\n",
    "            initial_rank = initial_ranks.get(disease, n_top+1)\n",
    "            final_rank = final_ranks.get(disease, n_top+1)\n",
    "            if abs(final_rank - initial_rank) > 2:  # Threshold for significant change\n",
    "                print(f\"\\n{disease_names[disease]} changed from rank {initial_rank+1} to {final_rank+1}\")\n",
    "\n",
    "# Run comparison\n",
    "compare_disease_rankings(model, essentials['disease_names'])\n",
    "def plot_signature_temporal_patterns(model, disease_names, n_top=10, selected_signatures=None):\n",
    "    \"\"\"\n",
    "    Show temporal patterns of top diseases for each signature\n",
    "    \"\"\"\n",
    "    #phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    #phi_avg = phi.mean(axis=2)  # Average over time\n",
    "\n",
    "     # Get phi and prevalence\n",
    "    phi = model.phi.detach().numpy()  # Shape: (K, D, T)\n",
    "    prevalence_logit = model.logit_prev_t.detach().numpy()  # Shape: (D, T)\n",
    "    \n",
    "    # Center phi relative to prevalence\n",
    "    phi_centered = np.zeros_like(phi)\n",
    "    for k in range(phi.shape[0]):\n",
    "        for d in range(phi.shape[1]):\n",
    "            phi_centered[k, d, :] = phi[k, d, :] - prevalence_logit[d, :]\n",
    "    \n",
    "    # Average over time\n",
    "    phi_avg = phi_centered.mean(axis=2)  # Shape: (K, D)\n",
    "    \n",
    "    # Select which signatures to plot\n",
    "    if selected_signatures is None:\n",
    "        selected_signatures = range(phi_avg.shape[0])\n",
    "    \n",
    "    # Create subplots for each selected signature\n",
    "    n_sigs = len(selected_signatures)\n",
    "    fig, axes = plt.subplots(n_sigs, 1, figsize=(15, 5*n_sigs))\n",
    "    if n_sigs == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, k in enumerate(selected_signatures):\n",
    "        # Get top diseases\n",
    "        scores = phi_avg[k, :]\n",
    "        top_indices = np.argsort(scores)[-n_top:][::-1]\n",
    "        \n",
    "        # Plot temporal patterns\n",
    "        ax = axes[i]\n",
    "        for idx in top_indices:\n",
    "            temporal_pattern = phi[k, idx, :]\n",
    "            ax.plot(temporal_pattern, label=disease_names[idx])\n",
    "        \n",
    "        ax.set_title(f'Signature {k} - Top Disease Temporal Patterns')\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Phi Value')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# First show the top diseases\n",
    "\n",
    "\n",
    "# Then show their temporal patterns\n",
    "# You can select specific signatures of interest:\n",
    "disease_names=essentials['disease_names']\n",
    "plot_signature_temporal_patterns(model, disease_names, selected_signatures=[0,1,14,15,16,13,17])\n",
    "Y_avg_global = torch.mean(torch.tensor(Y_global) if isinstance(Y_global, np.ndarray) else Y_global, dim=2)\n",
    "Y_avg_batch = torch.mean(torch.tensor(Y_100k) if isinstance(Y_100k, np.ndarray) else Y_100k, dim=2)\n",
    "\n",
    "def compare_disease_patterns(k, true_psi, Y_avg_global, Y_avg_batch):\n",
    "    \"\"\"Compare disease patterns between global and batch data\"\"\"\n",
    "    strong_diseases = (true_psi[k] > 0)\n",
    "    \n",
    "    # Get mean pattern for each dataset (average across patients)\n",
    "    pattern_large = Y_avg_global[:, strong_diseases].mean(dim=0)  # Average across all patients\n",
    "    pattern_small = Y_avg_batch[:, strong_diseases].mean(dim=0)  # Average across batch patients\n",
    "    \n",
    "    # Now both patterns are just length of strong_diseases\n",
    "    correlation = torch.corrcoef(\n",
    "        torch.stack([pattern_large, pattern_small])\n",
    "    )[0,1]\n",
    "    \n",
    "    print(f\"Signature {k} pattern correlation: {correlation:.3f}\")\n",
    "    print(f\"Mean pattern difference: {(pattern_large - pattern_small).abs().mean():.3f}\")\n",
    "\n",
    "# Now use it\n",
    "print(\"Comparing disease sharing patterns between global and batch data:\")\n",
    "for k in range(model.K):\n",
    "    print(f\"\\nSignature {k}:\")\n",
    "    compare_disease_patterns(k, model.psi, Y_avg_global, Y_avg_batch)\n",
    "model_save_path=\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'clusters': model.clusters,\n",
    "        'psi': model.psi,\n",
    "        'Y': Y_100k,\n",
    "        'prevalence_t': essentials['prevalence_t'],\n",
    "        'logit_prevalence_t': model.logit_prev_t,\n",
    "        'G': G_100k,\n",
    "        'E': E_100k,\n",
    "        'indices': indices,\n",
    "        'disease_names': disease_names,\n",
    "        'hyperparameters': {\n",
    "            'N': Y_100k.shape[0],\n",
    "            'D': Y_100k.shape[1],\n",
    "            'T': Y_100k.shape[2],\n",
    "            'P': G_100k.shape[1],\n",
    "            'K': model.phi.shape[0]\n",
    "        }\n",
    "    }, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env_pyro2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
