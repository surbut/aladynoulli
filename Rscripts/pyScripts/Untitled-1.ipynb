{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For iteration = 1 to max_iterations:\n",
    "\n",
    "    # Step 1: Update lambda_{ikt} using GP Classification\n",
    "    For each individual i:\n",
    "        For each category k:\n",
    "            # Extract the sequence of Z_ikt over time\n",
    "            Z_ik_t = Z[i][k][:]\n",
    "            # Perform GP classification to estimate lambda_ikt\n",
    "            lambda_ik_t = GP_classification(time_points, Z_ik_t)\n",
    "            # Update lambda\n",
    "            lambda[i][k][:] = lambda_ik_t\n",
    "\n",
    "    # Step 2: Compute theta_{ikt} using Softmax\n",
    "    For each individual i:\n",
    "        For each time t:\n",
    "            # Get lambda values for all categories at time t\n",
    "            lambda_i_t = lambda[i][:][t]  # Shape: (K,)\n",
    "            # Compute theta using softmax\n",
    "            theta[i][:][t] = softmax(lambda_i_t)\n",
    "\n",
    "    # Step 3: Compute Likelihoods and Sample Z_{ikt}\n",
    "    For each individual i:\n",
    "        For each time t:\n",
    "            For each category k:\n",
    "                # Compute log prior\n",
    "                log_prior = log(theta[i][k][t] + epsilon)\n",
    "                # Initialize log likelihood\n",
    "                log_likelihood = 0\n",
    "                For each dimension d:\n",
    "                    # Get observed data\n",
    "                    y_idt = y[i][d][t]\n",
    "                    # Get eta_{kdt} from previous iteration or initialization\n",
    "                    eta_kdt = eta[k][d][t]\n",
    "                    # Compute log likelihood contribution\n",
    "                    log_p = y_idt * log(eta_kdt + epsilon) + (1 - y_idt) * log(1 - eta_kdt + epsilon)\n",
    "                    log_likelihood += log_p\n",
    "                # Compute log posterior for category k\n",
    "                log_posterior[k] = log_prior + log_likelihood\n",
    "            # Normalize log posteriors\n",
    "            max_log_posterior = max(log_posterior)\n",
    "            unnorm_posteriors = exp(log_posterior - max_log_posterior)\n",
    "            posterior_probs = unnorm_posteriors / sum(unnorm_posteriors)\n",
    "            # Sample new category assignment\n",
    "            new_k = sample_categorical(posterior_probs)\n",
    "            # Update Z_{ikt}\n",
    "            Z[i][:][t] = 0  # Reset Z_{ikt} for all k\n",
    "            Z[i][new_k][t] = 1\n",
    "\n",
    "    # Step 4: Update phi_{kdt} using GP Classification\n",
    "    For each category k:\n",
    "        For each dimension d:\n",
    "            # Extract the sequence of s_{kdt} over time\n",
    "            s_kd_t = s[k][d][:]\n",
    "            # Perform GP classification to estimate phi_{kdt}\n",
    "            phi_kd_t = GP_classification(time_points, s_kd_t)\n",
    "            # Update phi\n",
    "            phi[k][d][:] = phi_kd_t\n",
    "\n",
    "    # Step 5: Compute eta_{kdt} using Sigmoid Function\n",
    "    For each category k:\n",
    "        For each dimension d:\n",
    "            For each time t:\n",
    "                phi_kdt = phi[k][d][t]\n",
    "                eta[k][d][t] = sigmoid(phi_kdt)\n",
    "\n",
    "    # Step 6: Compute Likelihoods and Sample s_{kdt}\n",
    "    For each category k:\n",
    "        For each dimension d:\n",
    "            For each time t:\n",
    "                # Get eta_{kdt}\n",
    "                eta_kdt = eta[k][d][t]\n",
    "                # Compute prior probabilities for s_{kdt} = 0 and s_{kdt} = 1\n",
    "                p_s1 = eta_kdt\n",
    "                p_s0 = 1 - eta_kdt\n",
    "                # Initialize log likelihoods\n",
    "                log_likelihood_s1 = 0\n",
    "                log_likelihood_s0 = 0\n",
    "                # For each individual i where Z_{ikt} = 1\n",
    "                For each individual i where Z[i][k][t] == 1:\n",
    "                    y_idt = y[i][d][t]\n",
    "                    # Likelihood when s_{kdt} = 1\n",
    "                    log_p_s1 = y_idt * log(1 - epsilon) + (1 - y_idt) * log(epsilon)\n",
    "                    log_likelihood_s1 += log_p_s1\n",
    "                    # Likelihood when s_{kdt} = 0\n",
    "                    log_p_s0 = y_idt * log(epsilon) + (1 - y_idt) * log(1 - epsilon)\n",
    "                    log_likelihood_s0 += log_p_s0\n",
    "                # Compute unnormalized posteriors\n",
    "                log_posterior_s1 = log(p_s1 + epsilon) + log_likelihood_s1\n",
    "                log_posterior_s0 = log(p_s0 + epsilon) + log_likelihood_s0\n",
    "                # Normalize posteriors\n",
    "                max_log_posterior = max(log_posterior_s1, log_posterior_s0)\n",
    "                unnorm_posterior_s1 = exp(log_posterior_s1 - max_log_posterior)\n",
    "                unnorm_posterior_s0 = exp(log_posterior_s0 - max_log_posterior)\n",
    "                sum_unnorm_posteriors = unnorm_posterior_s1 + unnorm_posterior_s0\n",
    "                posterior_p_s1 = unnorm_posterior_s1 / sum_unnorm_posteriors\n",
    "                # Sample s_{kdt}\n",
    "                s[k][d][t] = sample from Bernoulli(posterior_p_s1)\n",
    "\n",
    "    # Step 7: Check for Convergence (optional)\n",
    "    # If convergence criteria are met, break the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
